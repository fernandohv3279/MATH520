\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{enumitem}
\newcommand{\bld}[1]{\boldsymbol{#1}}

\begin{document}

\title{MATH520 Homework 3}
\author{Fernando}
\date{\today}
\maketitle

\section*{Exercise 8.18}
Let $f:\mathbb{R}^n\to\mathbb{R}$ be given by
$f(x)=\frac{1}{2}\bld{x}^T\bld{Q}\bld{x}-\bld{x}^Tb$, where
$\bld{b}\in\mathbb{R}^n$ and $\bld{Q}$ is a real symmetric positive definite
$n\times n$ matrix. Suppose that we apply the steepest descent method to this
function, with $\bld{x}^{(0)}\neq \bld{Q}^{-1}\bld{b}$. Show that the method
converges in one step, that is, $\bld{x}^{(1)}=\bld{Q}^{-1}\bld{b}$, if and
only if $\bld{x}^{(0)}$ is chosen such that
$\bld{g}^{(0)}=\bld{Q}\bld{x}^{(0)}=\bld{Q}\bld{x}^{(0)}-\bld{b}$ is an
eigenvector of $\bld{Q}$.
\subsection*{Solution}
TODO: CHECK HYPOTHESIS!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
First we note that $\nabla f(\bld{x})=\bld{Q}\bld{x}-\bld{b}$. And thus
\[
\bld{x}^{(1)}=\bld{x}^{(0)}-\alpha_0\nabla
f(\bld{x}^{(0)})=\bld{x}^{(0)}-\alpha_0(\bld{Q}\bld{x}^{(0)}-\bld{b}),
\]
Where $\alpha_0\neq 0$ because $\bld{x}^{(0)}\neq \bld{Q}^{-1}\bld{b}$.
Then
\begin{align*}
	\bld{x}^{(1)}&=\bld{Q}^{-1}\bld{b}\\
	\bld{x}^{(0)}-\alpha_0(\bld{Q}\bld{x}^{(0)}-\bld{b})&=\bld{Q}^{-1}\bld{b}\\
	\bld{Q}(\bld{x}^{(0)}-\alpha_0(\bld{Q}\bld{x}^{(0)}-\bld{b}))&=\bld{b}\\
	\bld{Q}(\bld{Q}\bld{x}^{(0)}-\bld{b}))&=\frac{1}{\alpha_0}(\bld{Q}\bld{x}^{(0)}-\bld{b}).
\end{align*}
\section*{Exercise 8.24}
Given $f:\mathbb{R}^n\to \mathbb{R}$, consider the general iterative algorithm
\[
	\bld{x}^{(k+1)}=\bld{x}^{(k)}+\alpha_k\bld{d}^{(k)},
\]
where $\bld{d}^{(1)},\bld{d}^{(2)},\dots$ are given vectors in $\mathbb{R}^n$
and $\alpha_k$ is chosen to minimize $f(\bld{x}^{(k)}+\alpha \bld{d}^{(k)})$;
that is,
\[
	\alpha_k = \arg \min f(\bld{x}^{(k)}+\alpha \bld{d}^{(k)}).
\]
Show that for each $k$, the vector $\bld{x}^{(k+1)}-\bld{x}^{(k)}$ is
orthogonal to $\nabla f(\bld{x}^{(k+1)})$ (assuming that the gradient exists).
\subsection*{Solution}
Let us define the function
\[
	h(\alpha)=f(\bld{x}^{(k)}+\alpha\bld{d}^{(k)})
\]
then
\[
	h'(\alpha)=\nabla f(x^{(k)}+\alpha \bld{d}^{(k)}) \cdot \bld{d}^{(k)}.
\]
By definition $\alpha_k$ satisfies $h'(\alpha_k)=0$ (FONC) so
\[
	0=h'(\alpha_k)=\nabla f(\bld{x}^{(k)}+\alpha_k \bld{d}^{(k)}) \cdot
	\bld{d}^{(k)} = \nabla f(\bld{x}^{(k+1)}) \cdot
	(\bld{x}^{(k+1)}-\bld{x}^{(k)})
\]
Which is what we wanted to prove.
\section*{Exercise 9.1}
Let $f:\mathbb{R} \to \mathbb{R}$ be given by $f(x) = (x-x_0)^4$, where $x_0\in \mathbb{R}$ is constant. Suppose that we apply Newton's method to the problem of minimizing $f$, with iterates $x^{(0)},x^{(1)},x^{(2)},\dots$
\begin{enumerate}[label=\alph*.]
\item Write down the update equation for Newton's method applied to the
problem.
\item Let $y^{(k)}=|x^{(k)}-x_0|$, where $x^{(k)}$ is the $k$th iterate in
	Newton's method. Show that the sequence $\{y^{(k)}\}$ satisfies
	$y^{(k+1)}=\frac{2}{3}y^{(k)}$.
\item Show that $x^{(k)}\to x_0$ for any initial guess $x^{(0)}$.
\item Show that the order of convergence of the sequence $\{x^{(k)}\}$ in part
	b is 1.
\item Theorem 9.1 states that under certain conditions, the order of
	convergence of Newton's method is at least 2. Why does that theorem not
	hold in this particular problem?
\end{enumerate}
\subsection*{Solution a}
in this case
\[
	\bld{F}(x)=12(x-x_0)^2
\]
so
\[
	\bld{F}(x^{(k)})^{-1}=\frac{1}{12(x^{(k)}-x_0)^2}
\]
and
\[
	\bld{g}^{(k)}=4(x^{(k)}-x_0)^3
\]
so
\[
	x^{(k+1)}=x^{(k)}-\frac{4(x^{(k)}-x_0)^3}{12(x^{(k)}-x_0)^2}=
	x^{(k)}-\frac{x^{(k)}-x_0}{3} = \frac{2x^{(k)}+x_0}{3}.
\]
\subsection*{Solution b}
\[
	y^{(k+1)}=|x^{(k+1)}-x_0|=\left|\frac{2x^{(k)}+x_0}{3}-x_0\right|
	=\left|\frac{2x^{(k)}-2x_0}{3}\right|=\frac{2}{3}y^{(k)}.
\]
\subsection*{Solution c}
This is equivalent to proving that $y^{(k)}\to 0$. From b we see that an
explicit formula for $y^{(k)}$ is $y^{(k)}=\left(\frac{2}{3}\right)^ky^{(0)}$.
Taking the limit as $k\to 0$ we get the result. Notice that the value of
$y^{(0)}$ is irrelevant.
\subsection*{Solution d}
We notice that
\begin{multline*}
	\lim_{k\to \infty} \frac{|x^{(k+1)}-x_0|}{|x^{(k)}-x_0|^p} = \lim_{k\to
	\infty}\frac{y^{(k+1)}}{\left(y^{(k)}\right)^p}=
\frac{2}{3}\lim_{k\to\infty}\frac{y^{(k)}}{(y^{(k)})^p}\\
= \frac{2}{3}\lim_{k\to\infty}(y^{(k)})^{1-p} =
\frac{2}{3}(y^{(0)})^{1-p}\lim_{k\to\infty}\left(\left(\frac{2}{3}\right)^{(1-p)}\right)^k.
\end{multline*}
Then we see that the biggest value $p$ can take is 1 because any $p$ bigger
than that would cause the limit to become infinite. With $p=1$ the limit is 1.
\subsection*{Solution e}
In this case $F(x^*)=0$ so $F(x^*)$ is not invertible and the theorem doesn't
hold.
\section*{Exercise 9.4}
Consider Rosenbrock's function: $f(\bld{x})=100(x_2-x_1^2)^2+(1-x_1)^2$, where
$\bld{x}=[x_1,x_2]^T$. This function is also known as the banana function
because of the shape of its level sets.
\begin{enumerate}[label=\alph*.]
\item Prove that $[1,1]^T$ is the unique global minimizer of $f$ over
	$\mathbb{R}^2$.
\item With a starting point of $[0,0]^T$, apply two iterations of Newton's
	method (with unit step size).
\item Repeat part b using a gradient algorithm with a fixed step size of
	$\alpha_k=0.05$ at each iteration.
\end{enumerate}
\subsection*{Solution a}
In this case
\[
	\nabla f(\bld{x})=[-400(x_2-x_1^2)x_1+2(x_1-1),200(x_2-x_1^2)]^T.
\]
and we can see that the only point that satisfies $\nabla f=0$ is $[1,1]^T$.
Also we have:
\[
	\bld{F}(x)=\begin{bmatrix}
		-400x_2 + 1200x_1^2 + 2 & -400x_1 \\
		-400x_1 & 200
	\end{bmatrix}
\]
and then the matrix $F([1,1]^T)$ is positive definite (I used WolframAlpha to
compute the eigenvalues) so $[1,1]^T$ is a unique minimizer.
\subsection*{Solution b}
\[
	\bld{x}^{(1)}=[0,0]^T - \frac{1}{200}
	\begin{bmatrix}
		100 & 0 \\
		0 & 1
	\end{bmatrix}
	\begin{bmatrix}
		-2\\
		0
	\end{bmatrix}
	=
	[1,0]^T
\]
\[
	\bld{x}^{(2)}=[1,0]^T - \frac{1}{40200}
	\begin{bmatrix}
		100 & 200 \\
		200 & 601
	\end{bmatrix}
	\begin{bmatrix}
		400\\
		-200
	\end{bmatrix}
	=
	[1,1]^T.
\]
\subsection*{Solution c}
\end{document}
